---
title: 'Lab week 11 solutions MAST90125: Bayesian Statistical learning'
header-includes:
   - \usepackage{bm}
   - \usepackage{amsmath}
output: 
  pdf_document:
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(2334576)
```

## Variational Bayes.

In lecture 20, we looked at (mean-field) Variational Bayes as a method to find approximate posterior distributions for sub-blocks of the parameter vector $\bm \theta$, for the model 

\begin{eqnarray}
p({\bf y}|\bm \beta, {\bf u}, \tau_e) &=& \mathcal{N}({\bf X}\bm \beta + {\bf Z}{\bf u}, \frac{1}{\tau_e}{\bf I}_n) \nonumber \\
p(\bm \beta) &\propto& 1   \nonumber \\
p({\bf u})   &=&\mathcal{N}({\bf 0}_q, \frac{1}{\tau_u}{\bf K}) \nonumber\\
p(\tau_e)    &=& \text{Ga}(\alpha_e,\gamma_e) \nonumber\\
p(\tau_u)    &=& \text{Ga}(\alpha_u,\gamma_u) \nonumber
\end{eqnarray}


## Instructions for lab

Download \texttt{farmdata.csv} and \texttt{Kmat.csv} from LMS.

Re-format the code given in lecture 20 for performing Variational Bayes so that rather than constructing approximate posteriors,

\begin{itemize}
\item $Q(\bm \beta)$
\item $Q({\bf u})$
\item $Q(\tau_e)$
\item $Q(\tau_u)$
\end{itemize}

you determine the approximate posteriors 

\begin{itemize}
\item $Q(\bm \beta, {\bf u})$
\item $Q(\tau_e)$
\item $Q(\tau_u)$.
\end{itemize}

Compare the performance of the new blocking structure to that used in class.


\subsection{Solution}

The difference between the code given in class and this lab is that we approximate the posterior of $\bm \beta, \bm u$ jointly. To do this, note the trick that the joint kernel of $\bm \beta, {\bf u}$ can be wirtten as 

\begin{eqnarray}
		p(\bm \beta,{\bf u}) \propto e^{-\frac{\tau_u\begin{tiny}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf 0}_{p \times p} & {\bf 0}_{p \times q} \\ {\bf 0}_{q \times p} & {\bf K}^{-1} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}\end{tiny}}{2}}, \nonumber
\end{eqnarray}

Hence in terms of coding, updating $\bm \beta, {\bf u}$ jointly looks just like updating $\bf u$ in the code given in lecture 19, except that $\bm \beta = {\bf 0}$.

As a reminder, the joint distribution, $p(y, {\bm \beta}, {\bf u}, \tau_e, \tau_u|{\bf X}, {\bf Z})$ is 
		\begin{eqnarray}
		\bigg( \frac{\tau_e}{2\pi}\bigg)^\frac{n}{2}e^{-\frac{\tau_e({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u})'({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u})}{2}} \hspace{-1mm}\times  \hspace{-1mm} 1  \hspace{-1mm} \times  \hspace{-1mm} \bigg(\frac{\tau_u}{2\pi}\bigg)^\frac{q}{2}\det({\bf K})^{-1/2} e^{-\frac{\tau_u{\bf u}'{\bf K}^{-1}{\bf u}}{2}}  \hspace{-1mm}\times  \hspace{-1mm} \frac{\gamma_u^{\alpha_u}\tau_u^{\alpha_u-1}e^{-\gamma_u\tau_u}}{\Gamma(\alpha_u)}  \hspace{-1mm}\times  \hspace{-1mm} \frac{\gamma_e^{\alpha_e}\tau_e^{\alpha_e-1}e^{-\gamma_e\tau_e}}{\Gamma(\alpha_e)}, \nonumber
		\end{eqnarray}
		
and that we need to determine the expectation of log-kernels.		

\begin{itemize}		
		\item For $\tau_e$, the kernel and log-kernel are respectively 

			\begin{eqnarray}
			\text{Kernel:}&&{\tau_e}^\frac{n}{2}e^{-\frac{\tau_e({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u})'({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u})}{2}}{\tau_e^{\alpha_e-1}e^{-\gamma_e\tau_e}}   \nonumber \\
			\text{Log-kernel:}&& (n/2+\alpha_e-1)\log(\tau_e) - \tau_e(\gamma_e +({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u})'({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u}) /2) \nonumber
			\end{eqnarray}

	\item The expected log-kernel $E_{-\tau_e}(\text{Log-kernel})$ is 

		\begin{eqnarray}
		\hspace*{-7 mm}(\frac{n}{2}+\alpha_e-1)\log(\tau_e) - \tau_e(\gamma_e +\frac{{\bf y}'{\bf y}+E_{\bm \beta, {\bf u} }\bigg (\begin{pmatrix}\bm \beta' & {\bf u}' \end{pmatrix}\begin{pmatrix}{\bf X}'{\bf X} & {\bf X}'{\bf Z}\\  {\bf Z}'{\bf X} & {\bf Z}'{\bf Z}\end{pmatrix}\begin{pmatrix}\bm \beta \\ {\bf u} \end{pmatrix} \bigg)}{2} - {\bf y}'{\bf X}E_{\bm\beta. {\bf u}}(\bm \beta) - {\bf y}'{\bf Z}E_{\bm \beta, \bf u}({\bf u}) ) \nonumber
		\end{eqnarray}
		
		
		\item Using the tricks that $E({\bf x}{\bf x}')= \text{Var}({\bf x}) + E({\bf x})E(\bf x)'$ and that ${\bf a}'{\bf D}{\bf a} = \text{Tr}({\bf a}'{\bf D}{\bf a})$ $= \text{Tr}({\bf D}{\bf a}{\bf a}')$, the expected log-kernel can be written as,		

			\begin{eqnarray}
			\hspace*{-30 mm}\bigg(\hspace{-1 mm}\frac{n}{2}+\alpha_e-1\hspace{-1 mm}\bigg)\hspace{-1 mm}\log(\tau_e)-\tau_e\bigg(\hspace{-1 mm}\gamma_e +\frac{({\bf y} - {\bf X}E_{\bm\beta, \bf u}(\bm \beta) - {\bf Z}E_{\bm \beta, \bf u}({\bf u}))'({\bf y} - {\bf X}E_{\bm\beta, \bf u}(\bm \beta)- {\bf Z}E_{\bm \beta, \bf u}({\bf u}))  +\text{Tr}\bigg(\begin{pmatrix}{\bf X}'{\bf X} & {\bf X}'{\bf Z}\\  {\bf Z}'{\bf X} & {\bf Z}'{\bf Z}\end{pmatrix}\begin{pmatrix}\text{Var}(\bm \beta) & \text{Var}(\bm \beta, {\bf u})\\  \text{Var}({\bf u},\bm \beta) & \text{Var}({\bf u})\end{pmatrix}\bigg)}{2} \hspace{-1 mm}\bigg),\nonumber
			\end{eqnarray}

	\item which indicates that the approximate posterior for $\tau_e$ is 

	\begin{eqnarray}
		\hspace*{-17 mm}\text{Ga}\bigg(\frac{n}{2}+\alpha_e,\gamma_e +\frac{({\bf y} - {\bf X}E_{\bm\beta, \bf u}(\bm \beta) - {\bf Z}E_{\bm \beta, \bf u}({\bf u}))'({\bf y} - {\bf X}E_{\bm\beta, \bf u}(\bm \beta)- {\bf Z}E_{\bm \beta, \bf u}({\bf u}))  +\text{Tr}\bigg(\begin{pmatrix}{\bf X}'{\bf X} & {\bf X}'{\bf Z}\\  {\bf Z}'{\bf X} & {\bf Z}'{\bf Z}\end{pmatrix}\begin{pmatrix}\text{Var}(\bm \beta) & \text{Var}(\bm \beta, {\bf u})\\  \text{Var}({\bf u},\bm \beta) & \text{Var}({\bf u})\end{pmatrix}\bigg)}{2}\bigg). \nonumber 
	\end{eqnarray}

	\item For $\tau_u$, the kernel and log-kernel are respectively 

		\begin{eqnarray}
		\text{Kernel:}&&\bigg(\frac{\tau_u}{2\pi}\bigg)^\frac{q}{2} e^{-\frac{\tau_u{\bf u}'{\bf K}^{-1}{\bf u}}{2}} {\tau_u^{\alpha_u-1}e^{-\gamma_u\tau_u}}  \nonumber \\
		\text{Log-kernel:}&& (q/2+\alpha_u-1)\log(\tau_u) - \tau_u(\gamma_u +{\bf u}'{\bf K}^{-1}{\bf u} /2) \nonumber
		\end{eqnarray}

	\item The expected log-kernel $E_{-\tau_u}(\text{Log-kernel})$ is 

		\begin{eqnarray}
		&=&(q/2+\alpha_u-1)\log(\tau_u) - \tau_u(\gamma_u +E_{\bm \beta, {\bf u}}({\bf u}'{\bf K}^{-1}{\bf u}) /2) \nonumber \\ &=&(q/2+\alpha_u-1)\log(\tau_u) - \tau_u(\gamma_u \text{Tr}({\bf K}^{-1}E_{\bm \beta, {\bf u}}({\bf u}{\bf u}')) /2) \nonumber \\
		&=& (q/2+\alpha_u-1)\log(\tau_u) - \tau_u(\gamma_u +E_{\bm \beta, {\bf u}}({\bf u})'{\bf K}^{-1}E_{\bm \beta, {\bf u}}({\bf u})/2 +\text{Tr}({\bf K}^{-1}Var({\bf u})) /2) \nonumber 
		\end{eqnarray}

	\item which indicates that the approximate posterior for $\tau_u$ is 

	\begin{eqnarray}
	\text{Ga}\bigg(\frac{q}{2}+\alpha_u,\gamma_u +\frac{E_{\bm \beta, {\bf u}}({\bf u})'{\bf K}^{-1}E_{\bm \beta, {\bf u}}({\bf u}) +\text{Tr}({\bf K}^{-1}Var({\bf u}))}{2}\bigg). \nonumber 
	\end{eqnarray}


		\item For $\bm \beta, \bf u$, the kernel and log-kernel are respectively 

			\begin{eqnarray}
			\text{Kernel:}&=& e^{-\frac{\tau_e({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u})'({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u})}{2}-\frac{\tau_u{\bf u}'{\bf K}^{-1}{\bf u}}{2}}\nonumber \\
			 &=& e^{-\frac{\tau_e({\bf y}  -  \begin{tiny}\begin{pmatrix} {\bf X} & {\bf Z} \end{pmatrix} \begin{pmatrix}\bm \beta \\ {\bf u} \end{pmatrix}\end{tiny})'({\bf y} -   \begin{tiny}\begin{pmatrix} {\bf X} & {\bf Z} \end{pmatrix} \begin{pmatrix}\bm \beta \\ {\bf u} \end{pmatrix}\end{tiny})}{2}}e^{-\frac{\tau_u\begin{tiny}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf 0}_{p \times p} & {\bf 0}_{p \times q} \\ {\bf 0}_{q \times p} & {\bf K}^{-1} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}\end{tiny}}{2}} \nonumber \\
			\text{Log-kernel:}&&-\frac{\tau_e({\bf y}  -  \begin{tiny}\begin{pmatrix} {\bf X} & {\bf Z} \end{pmatrix} \begin{pmatrix}\bm \beta \\ {\bf u} \end{pmatrix}\end{tiny})'({\bf y} -   \begin{tiny}\begin{pmatrix} {\bf X} & {\bf Z} \end{pmatrix} \begin{pmatrix}\bm \beta \\ {\bf u} \end{pmatrix}\end{tiny})}{2}-\frac{\tau_u\begin{tiny}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf 0}_{p \times p} & {\bf 0}_{p \times q} \\ {\bf 0}_{q \times p} & {\bf K}^{-1} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}\end{tiny}}{2} \nonumber
			\end{eqnarray}

		\item The expected log-kernel $E_{-\bm \beta, \bf u}(\text{Log-kernel})$ is 
	
			\begin{eqnarray}
			&=&-\frac{E_{\tau_e}(\tau_e)({\bf y}  -  \begin{tiny}\begin{pmatrix} {\bf X} & {\bf Z} \end{pmatrix} \begin{pmatrix}\bm \beta \\ {\bf u} \end{pmatrix}\end{tiny})'({\bf y} -   \begin{tiny}\begin{pmatrix} {\bf X} & {\bf Z} \end{pmatrix} \begin{pmatrix}\bm \beta \\ {\bf u} \end{pmatrix}\end{tiny})}{2}-\frac{E_{\tau_u}(\tau_u)\begin{tiny}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf 0}_{p \times p} & {\bf 0}_{p \times q} \\ {\bf 0}_{q \times p} & {\bf K}^{-1} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}\end{tiny}}{2} \nonumber \\
			&\propto&-\frac{E_{\tau_e}(\tau_e)\begin{tiny}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf X}'{\bf X} & {\bf X}'{\bf Z} \\{\bf Z}'{\bf X} & {\bf Z}'{\bf Z} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}\end{tiny}}{2} +E_{\tau_e}(\tau_e)\begin{footnotesize}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf X}'{\bf y}  \\{\bf Z}'{\bf y} \end{pmatrix}\end{footnotesize}-\frac{E_{\tau_u}(\tau_u)\begin{tiny}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf 0}_{p \times p} & {\bf 0}_{p \times q} \\ {\bf 0}_{q \times p} & {\bf K}^{-1} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}\end{tiny}}{2}  \nonumber \\
			&=&-\frac{\begin{tiny}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}E_{\tau_e}(\tau_e){\bf X}'{\bf X} & E_{\tau_e}(\tau_e){\bf X}'{\bf Z} \\E_{\tau_e}(\tau_e){\bf Z}'{\bf X} & E_{\tau_e}(\tau_e){\bf Z}'{\bf Z} + E_{\tau_u}(\tau_u){\bf K}^{-1} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}\end{tiny}}{2} +E_{\tau_e}(\tau_e)\begin{footnotesize}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf X}'{\bf y}  \\{\bf Z}'{\bf y} \end{pmatrix}\end{footnotesize} \nonumber 
			\end{eqnarray}

		
		\item which indicates that the approximate posterior for $\bm \beta, \bf u$ is 
			\begin{eqnarray}
			\mathcal{N}\bigg(E_{\tau_e}\begin{pmatrix}E_{\tau_e}(\tau_e){\bf X}'{\bf X} & E_{\tau_e}(\tau_e){\bf X}'{\bf Z} \\E_{\tau_e}(\tau_e){\bf Z}'{\bf X} & E_{\tau_e}(\tau_e){\bf Z}'{\bf Z} + E_{\tau_u}(\tau_u){\bf K}^{-1} \end{pmatrix}\begin{pmatrix}{\bf X}'{\bf y}  \\{\bf Z}'{\bf y} \end{pmatrix},\begin{pmatrix}E_{\tau_e}(\tau_e){\bf X}'{\bf X} & E_{\tau_e}(\tau_e){\bf X}'{\bf Z} \\E_{\tau_e}(\tau_e){\bf Z}'{\bf X} & E_{\tau_e}(\tau_e){\bf Z}'{\bf Z} + E_{\tau_u}(\tau_u){\bf K}^{-1} \end{pmatrix}^{-1}\bigg). \nonumber 
			\end{eqnarray}

	
    		
	\end{itemize}



```{r}
#Arguments are
#epsilon: accuracy cut-off.
#iter: no of iterations
#Kinv: inverse of K, where p(u) = N(0,\sigma^2_u K)
#Z: Predictor matrix for random effects
#X: Predictor matrix for fixed effects
#y: response vector
#taue_0: initial guess for residual precision.
#tauu_0: initial guess for random effect precision
#a.u, g.u: hyper-parameters of gamma prior for tauu
#a.e, g.e: hyper-parameters of gamma prior for taue

#Output are final estimates, plus iteration number when convergence was reached.
VB.mm<-function(epsilon,iter,Kinv,Z,X,y,taue_0,tauu_0,u0,beta0,a.e,g.e,a.u,g.u){
  n<-dim(X)[1]
  p<-dim(X)[2]
  q<-dim(Z)[2]
  W <-cbind(X,Z)
  WTW<-crossprod(W)
  WTY<-crossprod(W,y)
  Kinvall<-matrix(0,p+q,p+q)
  Kinvall[-c(1:p),-c(1:p)]<-Kinv
    
  for(i in 1:iter){
  Vub <-solve(taue_0*WTW+tauu_0*Kinvall) #update Var(b,u)
  ub  <-taue_0*Vub%*%WTY                 #update E(b,u)
  TrKinvub <- sum(diag(Kinvall%*%Vub))
  uKinvub  <- t(ub)%*%Kinvall%*%ub
  tauu    <- (a.u+0.5*q)/(g.u+0.5*as.numeric(uKinvub)+0.5*TrKinvub)
  tauu    <- as.numeric(tauu)
  err     <- y - W%*%ub
  TrWTWub  <- sum(diag(WTW%*%Vub))
  taue    <- (a.e+0.5*n)/(g.e+0.5*sum(err^2)+0.5*TrWTWub)
  taue    <- as.numeric(taue)
  
  if(i > 1){
  diffub  <- sqrt((ub-ub0)^2)/(abs(ub)+0.01)
  diffte <- abs(taue_0-taue)/(taue+0.01)
  difftu <- abs(tauu_0-tauu)/(tauu+0.01)
  diffvub <- sqrt((diag(Vub0) - diag(Vub))^2)/(diag(Vub))
  diff.all<-c(diffub,diffte,difftu,diffvub)
  if(max(diff.all) < epsilon) break
  }
  Vub0 <- Vub;ub0<-ub;taue_0<-taue;tauu_0<-tauu
  #Calculate relative change.
  }
  
  taue.param<-c((a.e+0.5*n),(g.e+0.5*sum(err^2)+0.5*TrWTWub))
  tauu.param<-c((a.u+0.5*q),(g.u+0.5*uKinvub+0.5*TrKinvub))  
  param<-list(ub,Vub,taue.param,tauu.param,i)
  names(param)<-c('betau_mean','betau_var','tau_e','tau_u','iter')
  return(param)
}
```

The R code used to implement Gibbs sampling for this mixed model in lecture 20 is given below:

```{r}
#Arguments are
#iter: no of iterations
#Z: Predictor matrix for random effects
#X: Predictor matrix for fixed effects
#y: response vector
#burnin: number of initial iterations to discard.
#taue_0: initial guess for residual precision.
#tauu_0: initial guess for random effect precision
#Kinv: inverse of K, where p(u) = N(0,\sigma^2_u K)
#a.u, b.u: hyper-parameters of gamma prior for tauu
#a.e, b.e: hyper-parameters of gamma prior for taue

normalmm.Gibbs<-function(iter,Z,X,y,burnin,taue_0,tauu_0,Kinv,a.u,b.u,a.e,b.e){
n   <-length(y) #no. observations
p   <-dim(X)[2] #no of fixed effect predictors.
q   <-dim(Z)[2] #no of random effect levels
tauu<-tauu_0
taue<-taue_0
beta0<-rnorm(p)
u0   <-rnorm(q,0,sd=1/sqrt(tauu))

#Building combined predictor matrix.
W<-cbind(X,Z)
WTW <-crossprod(W)
WTy <-crossprod(W,y)
library(mvtnorm)

#storing results.
par <-matrix(0,iter,p+q+2)
#Calculating log predictive densities
lppd<-matrix(0,iter,n)

#Create modified identity matrix for joint posterior.
I0  <-diag(p+q)
diag(I0)[1:p]<-0
I0[-c(1:p),-c(1:p)]  <-Kinv

for(i in 1:iter){
#Conditional posteriors.
  uKinvu <- t(u0)%*%Kinv%*%u0
  uKinvu <-as.numeric(uKinvu)
tauu <-rgamma(1,a.u+0.5*q,b.u+0.5*uKinvu)
#Updating component of normal posterior for beta,u
Prec <-WTW + tauu*I0/taue
P.mean<- solve(Prec)%*%WTy
P.var <-solve(Prec)/taue
betau <-rmvnorm(1,mean=P.mean,sigma=P.var)
betau <-as.numeric(betau)
err   <- y-W%*%betau
taue  <-rgamma(1,a.e+0.5*n,b.e+0.5*sum(err^2))
#storing iterations.
par[i,]<-c(betau,1/sqrt(tauu),1/sqrt(taue))
beta0  <-betau[1:p]
u0     <-betau[p+1:q]
lppd[i,]= dnorm(y,mean=as.numeric(W%*%betau),sd=1/sqrt(taue))
}

lppd      = lppd[-c(1:burnin),]
lppdest   = sum(log(colMeans(lppd)))        #Estimating lppd for whole dataset.
pwaic2    = sum(apply(log(lppd),2,FUN=var)) #Estimating effective number of parameters.
par <-par[-c(1:burnin),]
colnames(par)<-c(paste('beta',1:p,sep=''),paste('u',1:q,sep=''),'sigma_b','sigma_e')
mresult<-list(par,lppdest,pwaic2)
names(mresult)<-c('par','lppd','pwaic')
return(mresult)
}
```

Any other code you may use can be modified from the code given in Lecture 20. Possible things you may want to check include:

\begin{itemize}
\item convergence of Gibbs sampler
\item comparing the empirical and approximate distributions using density plots.
\end{itemize}


\paragraph{Importing and formatting the data}

```{r}
data<-read.csv('./farmdata.csv')
# data<-read.csv(file.choose())
K   <-read.csv('./Kmat.csv')
# K<-read.csv(file.choose())
K   <-as.matrix(K)
Kinv<-solve(K)

n<-dim(data)[1]
q<-dim(Kinv)[1]
X<-table(1:n,data$flock) #flock is fixed effect
#Indicator matrix for parents.
Z2<-table(1:n,data$sire)
Z3<-cbind(Z2,table(1:n,data$dam))
```


\paragraph{running the Gibbs sampler, checking convergence, and calculating effective sample size}

```{r}
system.time(chain1<-normalmm.Gibbs(iter=10000,Z=Z3,X=X,y=data$y,burnin=2000,taue_0=5, 
                  tauu_0=0.2,Kinv=Kinv,a.u=0.001,b.u=0.001,a.e=0.001,b.e=0.001))
system.time(chain2<-normalmm.Gibbs(iter=10000,Z=Z3,X=X,y=data$y,burnin=2000,taue_0=1, 
                  tauu_0=1,Kinv=Kinv,a.u=0.001,b.u=0.001,a.e=0.001,b.e=0.001))
system.time(chain3<-normalmm.Gibbs(iter=10000,Z=Z3,X=X,y=data$y,burnin=2000,taue_0=0.2, 
                  tauu_0=3,Kinv=Kinv,a.u=0.001,b.u=0.001,a.e=0.001,b.e=0.001))

library(coda)
rml1<-as.mcmc.list(as.mcmc((chain1$par[1:4000,])))
rml2<-as.mcmc.list(as.mcmc((chain2$par[1:4000,])))
rml3<-as.mcmc.list(as.mcmc((chain3$par[1:4000,])))
rml4<-as.mcmc.list(as.mcmc((chain1$par[4000+1:4000,])))
rml5<-as.mcmc.list(as.mcmc((chain2$par[4000+1:4000,])))
rml6<-as.mcmc.list(as.mcmc((chain3$par[4000+1:4000,])))
rml<-c(rml1,rml2,rml3,rml4,rml5,rml6)

#Gelman-Rubin diagnostic.
gelman.diag(rml)[[1]]
#effective sample size.
effectiveSize(rml)
```

\paragraph{Estimating parameters of variational Bayes approximations}
```{r}
system.time(test1<-VB.mm(epsilon=1e-5,iter=2000,Kinv=Kinv,Z=Z3,X=X,y=data$y,taue_0=0.2, 
      tauu_0=0.2,u0=rnorm(13),beta0=rnorm(2),a.e=0.001,g.e=0.001,a.u=0.001,g.u=0.001))
system.time(test2<-VB.mm(epsilon=1e-5,iter=2000,Kinv=Kinv,Z=Z3,X=X,y=data$y,taue_0=5, 
      tauu_0=1,u0=rnorm(13),beta0=rnorm(2),a.e=0.001,g.e=0.001,a.u=0.001,g.u=0.001))
system.time(test3<-VB.mm(epsilon=1e-5,iter=2000,Kinv=Kinv,Z=Z3,X=X,y=data$y,taue_0=1, 
      tauu_0=5,u0=rnorm(13),beta0=rnorm(2),a.e=0.001,g.e=0.001,a.u=0.001,g.u=0.001))

test1$iter
test2$iter
test3$iter
```

\paragraph{Comparing estimates obtained from Gibbs sampling and Variational Bayes}

```{r, fig1, fig.height = 25, fig.width = 20}
#Comparing point estimates/posterior means

chain.all<-rbind(chain1$par,chain2$par,chain3$par)
#beta, u
cbind(test1$betau_mean,test2$betau_mean,test3$betau_mean,colMeans(chain.all[,1:15]))


#Variances.
sigmae2<-sigmau2<-rep(0,3)
sigmae2[1]<-test1$tau_e[2]/(test1$tau_e[1]-1)
sigmau2[1]<-test1$tau_u[2]/(test1$tau_u[1]-1)
sigmae2[2]<-test2$tau_e[2]/(test2$tau_e[1]-1)
sigmau2[2]<-test2$tau_u[2]/(test2$tau_u[1]-1)
sigmae2[3]<-test3$tau_e[2]/(test3$tau_e[1]-1)
sigmau2[3]<-test3$tau_u[2]/(test3$tau_u[1]-1)
cbind(sigmau2,mean(chain.all[,16]^2))
cbind(sigmae2,mean(chain.all[,17]^2))

#Comparing posterior distributions.
par(mfrow=c(5,4))
mlim<-quantile(chain.all[,1],c(0.005,0.995))
curve(dnorm(x,mean=test1$betau_mean[1],sd=sqrt(test1$betau_var[1,1])),ylab='Density', 
    main='',xlim=mlim,col=2,lty=1,xlab=expression(beta[1]),cex.lab=2.5,cex.axis=1.5)
lines(density(chain.all[,1]))
legend('topright',legend=c('Gibbs','V-B'),col=1:2,lty=1,bty='n',cex=2.5)

mlim<-quantile(chain.all[,2],c(0.005,0.995))
curve(dnorm(x,mean=test1$betau_mean[2],sd=sqrt(test1$betau_var[2,2])),ylab='Density', 
  main='',xlim=mlim,col=2,lty=1,xlab=expression(beta[2]),cex.lab=2.5,cex.axis=1.5)
lines(density(chain.all[,2]))
legend('topright',legend=c('Gibbs','V-B'),col=1:2,lty=1,bty='n',cex=2.5)


#Repeat for random effects.
for(i in 1:13){ 
mlim<-quantile(chain.all[,i+2],c(0.005,0.995))
curve(dnorm(x,mean=test1$betau_mean[i+2],sd=sqrt(test1$betau_var[i+2,i+2])),ylab='Density', 
      main='',xlim=mlim,col=2,lty=1,xlab=paste('u',i,sep=''),cex.lab=2.5,cex.axis=1.5)
lines(density(chain.all[,i+2]))
legend('topright',legend=c('Gibbs','V-B'),col=1:2,lty=1,bty='n',cex=2.5)
}

mlim<-quantile(chain.all[,16]^2,c(0.005,0.995))
curve(dgamma(1/x,shape=test1$tau_u[1],rate=test1$tau_u[2])*x^(-2),ylab='Density',main='', 
    xlim=c(0,mlim[2]),col=2,lty=1,xlab=expression(sigma[u]^2),cex.lab=2.5,cex.axis=1.5)
lines(density(chain.all[,16]^2)) 
legend('topright',legend=c('Gibbs','V-B'),col=1:2,lty=1,bty='n',cex=2.5)
  
mlim<-quantile(chain.all[,17]^2,c(0.005,0.995))
curve(dgamma(1/x,shape=test1$tau_e[1],rate=test1$tau_e[2])*x^(-2),ylab='Density',main='', 
xlim=c(0,mlim[2]),col=2,lty=1,xlab=expression(sigma[e]^2),cex.lab=2.5,cex.axis=1.5,cex.axis=1.5)
lines(density(chain.all[,17]^2))
legend('topright',legend=c('Gibbs','V-B'),col=1:2,lty=1,bty='n',cex=2.5)
```

By determining the joint approximate posterior for $\bm \beta, {\bf u}$ rather than separate independent approximate posteriors for $\bm \beta$, $\bf u$ as in lecture 20, we improve the accuracy of approximate inference. This is because $\bf X$ and $\bf Z$ are not independent, and so contructing independent approximate posteriors for $\bm \beta$ and $\bf u$, we ignore the information contained in ${\bf X}'{\bf Z}$.
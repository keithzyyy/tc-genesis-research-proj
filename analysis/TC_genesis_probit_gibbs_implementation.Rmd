---
title: "TC_genesis_bayesian_analysis"
output: html_document
date: "2025-03-15"
---
#set seed
```{r}
set.seed(123)
```



# Working Directory Setup using `here()`
Use `here()` to *standardize the working directory between R markdown and
R console* as opposed to using `getwd()` (and `setwd()`), at the slight
inconveniences of importing the `here` library, using `here()` to write file
paths, and manually specify paths for files outside the working directory.

```{r setup, include=FALSE}
#install.packages('here')
library(here)
knitr::opts_chunk$set(echo = TRUE)
```

# Consolidate working directory & Imports
## Check working directory
```{r}
here()
```

## Import all libraries (incl. custom)
```{r}
source(here("lib", "preprocessing.R"))
library(mvtnorm)
library(truncnorm)
```


## Set path for data

```{r}
#list.files(here("data"))
```
```{r}
data_path = here("data", "envDataset_12h_10x10_with_mask.csv")
```

# Import & Preprocess the data
```{r}

# import the data
data = read.csv(data_path)

# process the data using preprocessing.R
data.processed = preprocess(data)
X = data.processed$predictors
y = data.processed$response


```

## Convert dataframes into a matrix + add intercept

```{r}
X = data.matrix(X)
X = cbind(matrix(1, nrow=nrow(X), ncol=1), X)

y = as.matrix(y)
```

```{r}
print(dim(X))
print(dim(y))
```

# Start the implementation

## Gibbs sampler function

```{r}
# Obtain samples from the posterior of beta and latent variable
# for Probit logistic regression with ridge prior with a fixed hyperparameter
# (lambda)
#
#
# @param X = the n x p design matrix, MUST include intercept column
# @param y = the response vector (a nx1 matrix)
# @param iter = number of iterations in the Gibbs sampler
# @param burnin = number of burn-in samples
# @param beta0 = initial values for the beta vectors. MUST be a px1 matrix.
# @param lambda = the ridge hyperparameter (fixed) 
#
# @returns a matrix of size (iter-burnin) x (p), where p is the 
# dimension of beta 

Probit.Gibbs.Sampler = function(X, y, iter, burnin, beta0, lambda){
  
  start.time.function = Sys.time()
  
  # 0. precalculate quantities for parameters of conditional dist
  p = ncol(X); n=nrow(X)
  XTX = crossprod(X)
  ridge.XTX.inv = solve( (XTX + lambda*diag(p)) )
  ridge.hat.matrix = ridge.XTX.inv %*% t(X) 
  samples = matrix(0, iter, p); colnames(samples) = colnames(X)
  beta = as.matrix(beta0)
  
  # for truncated normal samplings
  idx.pos = which(y == 1)
  idx.neg = which(y == 0)
  
  # due to numerical precision issues, we have 
  # to force the covariance matrix for cond. posterior of beta
  # to be symmetric.
  ridge.XTX.inv.corrected = 0.5 * (ridge.XTX.inv + t(ridge.XTX.inv))
  
  # begin the sampling process
  
  # track percentage of completion in increments of 10% of iter
  PROP.COMPLETE = 0
  for (i in 1:iter){
    
    if (i >= PROP.COMPLETE * iter){
      cat(100*PROP.COMPLETE,"% completed",
          " i.e. ", PROP.COMPLETE * iter, "iterations",
          "\n")
      PROP.COMPLETE = PROP.COMPLETE + 0.1
    } 
    
    
    
    # no need for z=as.matrix(numeric(n), nrow=n, ncol=1),
    # as R does allow multiplying a matrix by a vector
    z=numeric(n)
    
    # 1. sample z_1 ,.., z_n using its conditional distribution z_i^(i)|beta^(i-1), y
    
    # instead of looping through each z_i, use vectorized operations to 
    # sample in bulk for each binary category (y=1 and 0) and assign directly
    
    # for(j in 1:n){
    #  mu.j = X[j,] %*% beta
    #  
    #  if( y[j]==1 ){
    #    z[j] = rtruncnorm(1, a=0, b=Inf, mean=mu.j, sd=1)
    #  } else {
    #    z[j] = rtruncnorm(1, a=-Inf, b=0, mean=mu.j, sd=1)
    #  }
    #}
    
    # drop=FALSE is to keep it as a matrix/vector in case idx_pos or idx_neg
    # is only 1 element
    
    #cat("dim of X", dim(X), '\n')
    #cat("dim of beta", dim(beta) , '\n')
    
    mu = X %*% beta
    z[idx.pos] = rtruncnorm(length(idx.pos),
                            a=0, b=Inf, mean=mu[idx.pos, , drop=FALSE], sd=1)
    
    z[idx.neg] = rtruncnorm(length(idx.neg),
                            a=-Inf, b=0, mean=mu[idx.neg, , drop=FALSE], sd=1)
    
    # 2. sample beta from its conditional distribution beta^(i)|z_i^(i)
    beta = rmvnorm(1,
                   mean=as.vector(ridge.hat.matrix %*% z),
                   sigma=ridge.XTX.inv.corrected )
    
    # result is a row vector, need to reshape it into a col matrix
    # for future matrix operations.
    beta = t(beta)

    
    # 3. append the sampled beta
    samples[i,] = beta
    
    
  }
  
  end.time.function = Sys.time()
  print("\n")
  time.diff = end.time.function - start.time.function
  cat("Overall time taken (minutes): ", as.numeric(time.diff, units = "mins"))
  
  # remove first few samples as burnin
  return(samples[-c(1:burnin), ])
  
}

```


## First run
```{r}

chain1 = Probit.Gibbs.Sampler(X,y,iter=50000,
                     burnin=1000,
                     beta0=rnorm(ncol(X)),
                     lambda=1)

```


### Trace plots for high posterior means

For coefficients with large posterior means, are they large merely because of 
initial effects of burn-in?


```{r}

traceplots.high.coeffs = function(chain, magnitude='positive'){
  
  if (magnitude == 'negative'){
    top.feature.coeffs = names(sort(apply(chain, 2, mean), decreasing=FALSE))[1:6]
  } else{
    top.feature.coeffs = names(sort(apply(chain, 2, mean), decreasing=TRUE))[1:6]
  }

  par(mfrow = c(2, 3))
  
  for (top.feature in top.feature.coeffs){
    plot(1:nrow(chain), chain[,top.feature],
         main=paste0("trace plot for\n", top.feature),
         xlab=paste0("posterior mean= ", mean(chain[,top.feature]))
    )
    abline(v=5000)
  }
  
}

```



```{r}
traceplots.high.coeffs(chain1)
traceplots.high.coeffs(chain1, 'negative')
```


```{r}

EXTRA_BURN_IN = 5500

traceplots.high.coeffs(chain1[-c(1:EXTRA_BURN_IN),])
traceplots.high.coeffs(chain1[-c(1:EXTRA_BURN_IN),], 'negative')

```

### Saving samples from first chain 

```{r}
write.csv(chain1[-c(1:EXTRA_BURN_IN),],
          file="probit_chain1_without_checking.csv",
          row.names=FALSE)
```


### Practical issues encountered

- setting `lambda=0` leads to singularity of $X^TX$. No surprise since $p=157$ 
is large. So `lambda=1` instead

- decided to store posterior samples of $\beta$ only since storing the posterior
samples of the latent variables is apparently too big to fit to memory


- covariance parameter is apparently not symmetric for simulating cond. posterior
of $\beta$? Fix it to be symmetric by taking something like $0.5*(\Sigma + \Sigma^T)$

- Too long to run `Probit.Gibbs.Sampler(X,y,iter=100000, burnin=1000,beta0=rnorm(ncol(X)),
lambda=1)`. One bottleneck might be from sampling the latent variables, which
involved looping through EACH of the data points. So I vectorized the operations
to sample in bulk for each of $y=1$ and $y=0$. Also probably wise to reduce num
of iterations.

```{r}
ridge.hat.matrix.test = solve(crossprod(X) + diag(ncol(X)))
# how big is the difference anyway?
max(
  abs(
    ridge.hat.matrix.test - t(ridge.hat.matrix.test)
  )
)
```
```{r}
# inspecting covariance matrix of first few variables
ridge.hat.matrix.test[1:4,1:4]

```


## Second run

### Trace plot
```{r}
traceplots.high.coeffs(chain2)
traceplots.high.coeffs(chain2, 'negative')
```

```{r}

chain2 = Probit.Gibbs.Sampler(X,y,iter=50000,
                     burnin=7000,
                     beta0=rnorm(ncol(X), mean=1.5),
                     lambda=1)

```
### Saving samples from 2nd chain 

```{r}
write.csv(chain2[-c(1:EXTRA_BURN_IN),],
          file="probit_chain2_without_checking.csv",
          row.names=FALSE)
```

## Third run

```{r}
chain3 = Probit.Gibbs.Sampler(X,y,iter=50000,
                     burnin=7000,
                     beta0=rnorm(ncol(X), mean=-1.5),
                     lambda=1)
```

### Trace plot
```{r}
traceplots.high.coeffs(chain3)
traceplots.high.coeffs(chain3, 'negative')
```


### Saving samples from 3rd chain 

```{r}
write.csv(chain3[-c(1:EXTRA_BURN_IN),],
          file="probit_chain3_without_checking.csv",
          row.names=FALSE)
```








